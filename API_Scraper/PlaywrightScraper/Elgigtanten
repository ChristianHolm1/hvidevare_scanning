import asyncio
import json
from playwright.async_api import async_playwright
from bs4 import BeautifulSoup
from tqdm.asyncio import tqdm
import logging
import os

# Configure logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

# Asynchronous function to fetch page content
async def fetch_page_content(url, user_agent, semaphore):
    async with semaphore, async_playwright() as p:
        try:
            browser = await p.chromium.launch()
            context = await browser.new_context(user_agent=user_agent, viewport={'width': 1280, 'height': 720})
            page = await context.new_page()
            logging.info(f"Fetching URL: {url}")
            await page.goto(url, wait_until="networkidle", timeout=30000)


            cookie_accept_button_selector = '.coi-banner__accept'
            if await page.query_selector(cookie_accept_button_selector):
                await page.click(cookie_accept_button_selector)

            title_selector = '.product-title' 
            product_title = await page.text_content(title_selector)
            screenshot_filename = os.path.join(r"Hvidevare scanning\API_Scraper\PlaywrightScraper\ProductData\Elgiganten\Screenshots\Elgiganten", f"{product_title}.png")
            await page.screenshot(path=screenshot_filename)
            content = await page.content()
           
            await browser.close()
            return content
        except Exception as e:
            logging.error(f"Error fetching URL {url}: {e}")
            return None



from bs4 import BeautifulSoup


def parse_product_page(html_content):
    soup = BeautifulSoup(html_content, 'html.parser')
    url = soup.find('link', rel='canonical').get('href')
    title = soup.find('h1', class_='product-title').get_text(strip=True)
    price_span = soup.find('span', class_='price__value')
    price = price_span.find('span').get_text(strip=True) if price_span else 'No Price'
    

    return {'url': url, 'title': title, 'price': price}




async def main():
    user_agent = "Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:109.0) Gecko/20100101 Firefox/119.0"
    scraped_data = []

    with open(r'C:\Users\Lucas\OneDrive\Documents\Project Solita\Hvidevare scanning\API_Scraper\ScrapyScraper\data\ElgigantenSpider_20231114_082904.json', 'r') as file:
        data = json.load(file)


    concurrency_limit = 10 
    semaphore = asyncio.Semaphore(concurrency_limit)

 
    scrape_tasks = [asyncio.create_task(fetch_page_content(item['url'], user_agent, semaphore)) for item in data]

    for task in tqdm.as_completed(scrape_tasks, total=len(scrape_tasks)):
        try:
            html_content = await task
            if html_content:
                product_info = parse_product_page(html_content)
                scraped_data.append(product_info)
        except Exception as e:
            logging.error(f"Error processing task: {e}")


    with open('scraped_data.json', 'w') as file:
        json.dump(scraped_data, file, indent=4)


if __name__ == "__main__":
    asyncio.run(main())
